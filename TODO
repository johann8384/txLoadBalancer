Roadmap
=======

Bugs
----


Placeholders
------------
 * update README with example API usage
 * add a tool for generating a password (bin script that calls util functions)
 * can nextHost() get unburried? How "close" can we bring it to the receiver
   factory?
 * rename 'manager' attributes to 'taskManager'
 * rename 'director' attributes to 'proxyManager'


Version 1.2.0
-------------
 * replace all the copious uses of del with a better idiom
 * add a twisted plugin for twistd where you can specify something like an --lb
   option
   . this would provide the ability for a single code base to act in either
     proxied host (serving content) mode or manager (directing requests to the
     the proxied hosts)
   . need to look at the possibility of using Ampoule
   . what about code? at the very least, all the dependencies will have to be
     met on the remote hosts, but what about txLB itself? create a tarball and
     send it over? In any case, an --lb-basedir= option will be required as
     well...
   . legal values could include "manager" and "proxied"
   . if --lb was omitted, "proxied" would be default
 * add improved support for self-load-balancing apps:
   . manager pings remote hosts to see if up
   . manager then attempts to deploy clones of itself to remove hosts
   . manager then attemptes to start remote clones
   . when clones start up, they perform host checks on themsevles and see what
     they are supposed to be running (and if they are the manager or a proxied
     host)
 * add a scheduler that accounts for memory and CPU utilization, which will
   require:
   . a standard python means for services to get system memory and other
     resource utilization stats
   . a means for services to report this to the ProxyManager
   . an algorigthm (or series of them) for determining the suitablity of a
     service to receive the next request based on a total evalution of current
     reported resource utilization
 * improve the performance of the weighted scheduler
   . provide easy access to the host weights to the tracker
   . create a custom caching/purging scheduler for calculated host weight
     populations
 * add an API for use by any scheduler that needs caching, providing the
   requisite data to any part of txLB that needs it (tracker, pm/director,
   admin UI)
 * update application.service.LoadBalancedService to check for lb type:
   . if the lb type needs a custom scheduler (e.g., for caching values), add
     that scheduler as a service automatically with default interval
 * sort service groups in admin UI by name
 * cache DNS lookups
 * continue adding unit tests
 * continue adding any missing docstring
 * support a configurable option to log connections that are made to the
   proxied hosts


Version 1.3.0
-------------
 * add heartbeat/HA features for lb'ed services to communicate back up to the
   lb manager
   . use a TCP pinger to hit all hosts on a regular basis
   . take them out of ratation if unreachable
   . only put them back into rotation if they are pingable again
   . clones (proxied hosts) also ping back to the server to make sure they are
     still up; if it is down, one of the proxies can take over as manager
 * add an XML-RPC service instead of the custom web API
   . including adding methods for bouncing the admin UI
 * replace old PyDirector logging with Twisted logging
 * figure out a way to restart/upgrade code. Idea: close down listener, start
   new code immediately
   . let old connections on old pydir finish (run to completion) before exiting
   . look at (again) JP's live process migration work from several years ago
 * drop as much of config.__init__ as possible and map XML directly to model
   objects instead
 * move CSS into a static file
 * continue adding unit tests


Version 1.4.0
-------------
 * [maybe] make the "top" link point to a list of service groups, linked to
   pages that just display that one section (as opposed to the view that
   "running" gives, which is all sections)
 * examine the benefits of schedulers having write control over host data
   . all that information is connection-oriented, and thogh the schedulers need
     that info, it is not part of their purpose: determining where the current
     request should be directed
   . connection information should be at the proxy level (the TCP server
     that opens connections to the proxied host)
   . proxies should track information for each of its connections (really, the
     connections being made to the proxy's receiver)
 * examine breaking out listeners and managers across different processes/hosts
   . if a Twisted service is to be loadbalanced how should that happen?
   . if we are running a manger daemon for twisted services that are configured
     to be loadbalanced, how should that happen?
   . what about all of the services potentially being equal? any can be a
     manager, any can be balanced via another services proxy manager
 * sticky connections. keep a cache of 'client_ip':'backend_ip'
   . manager needs to be able to call "scheduler.cleanup()" to allow the
     cache to be manageable.
 * continue adding unit tests


Version 1.5.0
-------------
 * convert the user auth to use cred
 * complete unit tests


Version 2.0.0
-------------
 * the AMP-based "bottle" protocol
 * messaging, process control
 * cluster management
 * add additional authentication support
   . LDAP
   . RADIUS
 * connection count scheduler
  . could be used to track licensing of a service, or limit it


Exploratory
-----------
 * develop a twisted.application API that can be used by other applications
   to load balancing
   . a service needs to indicate that it is load balancing
   . if it is, it needs to read its load balancing configuration object
   . it needs to start up one or more proxies
   . it needs shutdown its main service and replace it with a listening service
 * configuration-oriented methods need to be moved onto new objects in a module
   of the txlb.application sub package.
 * static load balancring service
   . number of proxies are determined by configuration
 * dynamic load balancing service
   . number of proxies are determined by concurrent requests/load/etc
   . need to define "load"
 * cross-host-messaging
   . copies of a user's appication get deployed
   . each instance checks to see if it's the manaing host (hostname check)
   . those that aren't in the list of proxied hosts quit
   . those that are, actve as the simple service
   . the host (just one, for now) that is the manager starts up the lb service
 * possibly add proxy mixins/subclasses for different protocols (HTTP, DNS,
   etc.)
 * add the ability to do auto-naming (auto-incrementing a counter) so that if a
   name is left out (or if a host is auto-detected) it has a default
 * add a live upgrader:
   . developer builds an upgrader package (not necessarily a Python package)
   . package gets placed in a directory that txLoadBalancer regularly scans
   . the new package is placed into a time-based supdirectory that is made into
     a Python package (by giving it an __init__.py); as such, there will never
     be two overlapping upgrader Python packages
   . the upgrader gets imported and the resident memory objects are monkey
     patched, while the code on the filesystem is replaced
 * add the ability to dynamically add hosts into a proxy's rotation (the
   scheduler needs to be able to see them)
 * for applications that needed a crazy level of assured uptime, we could
   create load-balanced services on the fly that would allow them to migrate
   a service from one host to another simply by having both proxied, and when
   the old one goes down, it's taken out of the rotation

